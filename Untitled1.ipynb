{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSwM7p9NqjT1Whkgn5Pe+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitaniK/seq2seq/blob/test-branch/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "XrC2SaQ45j3W",
        "outputId": "444bd27f-7cd5-4055-8a63-240d9a8a1bc4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d03a27262a69>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import joblib\n",
        "import config\n",
        "\n",
        "\n",
        "class VideoDescriptionTrain(object):\n",
        "    \"\"\"\n",
        "    Initialize the parameters for the model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.train_path = config.train_path\n",
        "        self.test_path = config.test_path\n",
        "        self.max_length = config.max_length\n",
        "        self.batch_size = config.batch_size\n",
        "        self.lr = config.learning_rate\n",
        "        self.epochs = config.epochs\n",
        "        self.latent_dim = config.latent_dim\n",
        "        self.validation_split = config.validation_split\n",
        "        self.num_encoder_tokens = config.num_encoder_tokens\n",
        "        self.num_decoder_tokens = config.num_decoder_tokens\n",
        "        self.time_steps_encoder = config.time_steps_encoder\n",
        "        self.time_steps_decoder = None\n",
        "        self.x_data = {}\n",
        "\n",
        "        # processed data\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # models\n",
        "        self.encoder_model = None\n",
        "        self.decoder_model = None\n",
        "        self.inf_encoder_model = None\n",
        "        self.inf_decoder_model = None\n",
        "        self.save_model_path = config.save_model_path\n",
        "\n",
        "    def preprocessing(self):\n",
        "        \"\"\"\n",
        "        Preprocessing the data\n",
        "        dumps values of the json file into a list\n",
        "        \"\"\"\n",
        "        TRAIN_LABEL_PATH = os.path.join(self.train_path, 'training_label.json')\n",
        "        with open(TRAIN_LABEL_PATH) as data_file:\n",
        "            y_data = json.load(data_file)\n",
        "        train_list = []\n",
        "        vocab_list = []\n",
        "        for y in y_data:\n",
        "            for caption in y['caption']:\n",
        "                caption = \"<bos> \" + caption + \" <eos>\"\n",
        "                if len(caption.split()) > 10 or len(caption.split()) < 6:\n",
        "                    continue\n",
        "                else:\n",
        "                    train_list.append([caption, y['id']])\n",
        "\n",
        "        random.shuffle(train_list)\n",
        "        training_list = train_list[int(len(train_list) * self.validation_split):]\n",
        "        validation_list = train_list[:int(len(train_list) * self.validation_split)]\n",
        "        for train in training_list:\n",
        "            vocab_list.append(train[0])\n",
        "        self.tokenizer = Tokenizer(num_words=self.num_decoder_tokens)\n",
        "        self.tokenizer.fit_on_texts(vocab_list)\n",
        "\n",
        "        TRAIN_FEATURE_DIR = os.path.join(self.train_path, 'feat')\n",
        "        for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
        "            f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename), allow_pickle=True)\n",
        "            self.x_data[filename[:-4]] = f\n",
        "        return training_list, validation_list\n",
        "\n",
        "    def load_dataset(self, training_list):\n",
        "        \"\"\"\n",
        "        Loads the dataset in batches for training\n",
        "        :return: batch of data\n",
        "        \"\"\"\n",
        "        encoder_input_data = []\n",
        "        decoder_input_data = []\n",
        "        decoder_target_data = []\n",
        "        videoId = []\n",
        "        videoSeq = []\n",
        "        for idx, cap in enumerate(training_list):\n",
        "            caption = cap[0]\n",
        "            videoId.append(cap[1])\n",
        "            videoSeq.append(caption)\n",
        "        train_sequences = self.tokenizer.texts_to_sequences(videoSeq)\n",
        "        train_sequences = np.array(train_sequences)\n",
        "        train_sequences = pad_sequences(train_sequences, padding='post', truncating='post',\n",
        "                                        maxlen=self.max_length)\n",
        "        file_size = len(train_sequences)\n",
        "        n = 0\n",
        "        for i in range(self.epochs):\n",
        "            for idx in range(0, file_size):\n",
        "                n += 1\n",
        "                encoder_input_data.append(self.x_data[videoId[idx]])\n",
        "                y = to_categorical(train_sequences[idx], self.num_decoder_tokens)\n",
        "                decoder_input_data.append(y[:-1])\n",
        "                decoder_target_data.append(y[1:])\n",
        "                if n == self.batch_size:\n",
        "                    encoder_input = np.array(encoder_input_data)\n",
        "                    decoder_input = np.array(decoder_input_data)\n",
        "                    decoder_target = np.array(decoder_target_data)\n",
        "                    encoder_input_data = []\n",
        "                    decoder_input_data = []\n",
        "                    decoder_target_data = []\n",
        "                    n = 0\n",
        "                    yield ([encoder_input, decoder_input], decoder_target)\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"\n",
        "        an encoder decoder sequence to sequence model\n",
        "        reference : https://arxiv.org/abs/1505.00487\n",
        "        \"\"\"\n",
        "        encoder_inputs = Input(shape=(config.time_steps_encoder, config.num_encoder_tokens), name=\"encoder_inputs\")\n",
        "        encoder = LSTM(config.latent_dim, return_state=True, return_sequences=True, name='encoder_lstm')\n",
        "        _, state_h, state_c = encoder(encoder_inputs)\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "        decoder_inputs = Input(shape=(config.time_steps_decoder, config.num_decoder_tokens), name=\"decoder_inputs\")\n",
        "        decoder_lstm = LSTM(config.latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "        decoder_dense = Dense(config.num_decoder_tokens, activation='relu', name='decoder_relu')\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "        # model.summary()\n",
        "        training_list, validation_list = self.preprocessing()\n",
        "\n",
        "        train = self.load_dataset(training_list)\n",
        "        valid = self.load_dataset(validation_list)\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='min')\n",
        "\n",
        "        # Run training\n",
        "        opt = keras.optimizers.Adam(lr=0.0003)\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                      factor=0.1, patience=5, verbose=0,\n",
        "                                                      mode=\"auto\")\n",
        "        model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n",
        "\n",
        "        validation_steps = len(validation_list)//self.batch_size\n",
        "        steps_per_epoch = len(training_list)//self.batch_size\n",
        "\n",
        "        model.fit(train, validation_data=valid, validation_steps=validation_steps,\n",
        "                  epochs=self.epochs, steps_per_epoch=steps_per_epoch,\n",
        "                  callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "        if not os.path.exists(self.save_model_path):\n",
        "            os.makedirs(self.save_model_path)\n",
        "\n",
        "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
        "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
        "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "            decoder_inputs, initial_state=decoder_states_inputs)\n",
        "        decoder_states = [state_h, state_c]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        self.decoder_model = Model(\n",
        "            [decoder_inputs] + decoder_states_inputs,\n",
        "            [decoder_outputs] + decoder_states)\n",
        "        # self.encoder_model.summary()\n",
        "        # self.decoder_model.summary()\n",
        "\n",
        "        # saving the models\n",
        "        self.encoder_model.save(os.path.join(self.save_model_path, 'encoder_model.h5'))\n",
        "        self.decoder_model.save_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n",
        "        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'wb') as file:\n",
        "            joblib.dump(self.tokenizer, file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_to_text = VideoDescriptionTrain(config)\n",
        "    video_to_text.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yK-A6euV3T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git add hello.py\n",
        "! git commit -m \"[add] hello.py\"\n",
        "! git push origin {BRANCH}:{BRANCH}\n"
      ],
      "metadata": {
        "id": "p-xGbc6kVMOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2Zvpx3rVS_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}